{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying routine\n",
    "\n",
    "def display_images(in_, out, n=1, label=None, count=False):\n",
    "    for N in range(n):\n",
    "        if in_ is not None:\n",
    "            in_pic = in_.data.cpu().view(-1, 28, 28)\n",
    "            plt.figure(figsize=(18, 4))\n",
    "            plt.suptitle(label + ' – real test data / reconstructions', color='k', fontsize=16)\n",
    "            for i in range(4):\n",
    "                plt.subplot(1,4,i+1)\n",
    "                plt.imshow(in_pic[i+4*N])\n",
    "                plt.axis('off')\n",
    "        out_pic = out.data.cpu().view(-1, 28, 28)\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        for i in range(4):\n",
    "            plt.subplot(1,4,i+1)\n",
    "            plt.imshow(out_pic[i+4*N])\n",
    "            plt.axis('off')\n",
    "            if count: plt.title(str(4 * N + i), color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loading step\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MNIST('./data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the device\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "\n",
    "d = 20\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, d * 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, 784),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.new_empty(std.size()).normal_()\n",
    "            # eps = std.mul(0.5).new_empty(std.size()).cauchy_()\n",
    "            # eps = std.new_empty(std.size()).log_normal_()\n",
    "            # print(eps.size())\n",
    "            return eps.mul_(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_logvar = self.encoder(x.view(-1, 784)).view(-1, 2, d)\n",
    "        # print(mu_logvar.size())\n",
    "        mu = mu_logvar[:, 0, :]\n",
    "        logvar = mu_logvar[:, 1, :]\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the optimiser\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + β * KL divergence losses summed over all elements and batch\n",
    "\n",
    "def loss_function(x_hat, x, mu, logvar, β=1):\n",
    "    BCE = nn.functional.binary_cross_entropy(\n",
    "        x_hat, x.view(-1, 784), reduction='sum'\n",
    "    )\n",
    "    KLD = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))\n",
    "\n",
    "    return BCE + β * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 544.4361\n",
      "====> Epoch: 1 Average loss: 305.9237\n",
      "====> Test set loss: 227.4890\n",
      "====> Epoch: 2 Average loss: 208.3150\n",
      "====> Test set loss: 186.3266\n",
      "====> Epoch: 3 Average loss: 180.2653\n",
      "====> Test set loss: 164.8032\n",
      "====> Epoch: 4 Average loss: 165.9385\n",
      "====> Test set loss: 153.3235\n",
      "====> Epoch: 5 Average loss: 156.9013\n",
      "====> Test set loss: 144.8304\n",
      "====> Epoch: 6 Average loss: 149.5681\n",
      "====> Test set loss: 137.0703\n",
      "====> Epoch: 7 Average loss: 142.9340\n",
      "====> Test set loss: 131.0837\n",
      "====> Epoch: 8 Average loss: 137.9513\n",
      "====> Test set loss: 126.3503\n",
      "====> Epoch: 9 Average loss: 134.2326\n",
      "====> Test set loss: 122.8840\n",
      "====> Epoch: 10 Average loss: 131.3859\n",
      "====> Test set loss: 120.4210\n",
      "====> Epoch: 11 Average loss: 129.0874\n",
      "====> Test set loss: 118.1632\n",
      "====> Epoch: 12 Average loss: 127.1412\n",
      "====> Test set loss: 116.5187\n",
      "====> Epoch: 13 Average loss: 125.4158\n",
      "====> Test set loss: 114.8894\n",
      "====> Epoch: 14 Average loss: 123.8698\n",
      "====> Test set loss: 113.4098\n",
      "====> Epoch: 15 Average loss: 122.5306\n",
      "====> Test set loss: 112.3282\n",
      "====> Epoch: 16 Average loss: 121.3295\n",
      "====> Test set loss: 110.8189\n",
      "====> Epoch: 17 Average loss: 120.3113\n",
      "====> Test set loss: 110.0487\n",
      "====> Epoch: 18 Average loss: 119.3693\n",
      "====> Test set loss: 109.0693\n",
      "====> Epoch: 19 Average loss: 118.5448\n",
      "====> Test set loss: 108.3771\n",
      "====> Epoch: 20 Average loss: 117.8354\n",
      "====> Test set loss: 107.9291\n",
      "====> Epoch: 21 Average loss: 117.1017\n",
      "====> Test set loss: 106.9948\n",
      "====> Epoch: 22 Average loss: 116.4854\n",
      "====> Test set loss: 106.5869\n",
      "====> Epoch: 23 Average loss: 115.9183\n",
      "====> Test set loss: 105.8471\n",
      "====> Epoch: 24 Average loss: 115.3585\n",
      "====> Test set loss: 105.2740\n",
      "====> Epoch: 25 Average loss: 114.8575\n",
      "====> Test set loss: 105.0297\n",
      "====> Epoch: 26 Average loss: 114.3878\n",
      "====> Test set loss: 104.5328\n",
      "====> Epoch: 27 Average loss: 113.9605\n",
      "====> Test set loss: 104.0076\n",
      "====> Epoch: 28 Average loss: 113.5677\n",
      "====> Test set loss: 103.7192\n",
      "====> Epoch: 29 Average loss: 113.1961\n",
      "====> Test set loss: 103.4588\n",
      "====> Epoch: 30 Average loss: 112.8139\n",
      "====> Test set loss: 103.1348\n",
      "====> Epoch: 31 Average loss: 112.4775\n",
      "====> Test set loss: 102.7999\n",
      "====> Epoch: 32 Average loss: 112.1673\n",
      "====> Test set loss: 102.6140\n",
      "====> Epoch: 33 Average loss: 111.8731\n",
      "====> Test set loss: 102.1360\n",
      "====> Epoch: 34 Average loss: 111.5836\n",
      "====> Test set loss: 101.8517\n",
      "====> Epoch: 35 Average loss: 111.3296\n",
      "====> Test set loss: 101.6133\n",
      "====> Epoch: 36 Average loss: 111.0572\n",
      "====> Test set loss: 101.2406\n",
      "====> Epoch: 37 Average loss: 110.8308\n",
      "====> Test set loss: 101.3184\n",
      "====> Epoch: 38 Average loss: 110.6109\n",
      "====> Test set loss: 100.9603\n",
      "====> Epoch: 39 Average loss: 110.3801\n",
      "====> Test set loss: 100.7900\n",
      "====> Epoch: 40 Average loss: 110.1871\n",
      "====> Test set loss: 100.7273\n",
      "====> Epoch: 41 Average loss: 110.0129\n",
      "====> Test set loss: 100.5582\n",
      "====> Epoch: 42 Average loss: 109.7767\n",
      "====> Test set loss: 100.0922\n",
      "====> Epoch: 43 Average loss: 109.6459\n",
      "====> Test set loss: 100.0247\n",
      "====> Epoch: 44 Average loss: 109.4330\n",
      "====> Test set loss: 99.7130\n",
      "====> Epoch: 45 Average loss: 109.2607\n",
      "====> Test set loss: 99.8741\n",
      "====> Epoch: 46 Average loss: 109.1455\n",
      "====> Test set loss: 99.6446\n",
      "====> Epoch: 47 Average loss: 109.0434\n",
      "====> Test set loss: 99.4933\n",
      "====> Epoch: 48 Average loss: 108.8544\n",
      "====> Test set loss: 99.5223\n",
      "====> Epoch: 49 Average loss: 108.7150\n",
      "====> Test set loss: 99.1974\n",
      "====> Epoch: 50 Average loss: 108.5929\n",
      "====> Test set loss: 99.3073\n",
      "====> Epoch: 51 Average loss: 108.4410\n",
      "====> Test set loss: 99.0075\n",
      "====> Epoch: 52 Average loss: 108.3191\n",
      "====> Test set loss: 99.0137\n",
      "====> Epoch: 53 Average loss: 108.2187\n",
      "====> Test set loss: 98.8054\n",
      "====> Epoch: 54 Average loss: 108.0787\n",
      "====> Test set loss: 98.8107\n",
      "====> Epoch: 55 Average loss: 107.9562\n",
      "====> Test set loss: 98.3378\n",
      "====> Epoch: 56 Average loss: 107.8664\n",
      "====> Test set loss: 98.5816\n",
      "====> Epoch: 57 Average loss: 107.7209\n",
      "====> Test set loss: 98.3930\n",
      "====> Epoch: 58 Average loss: 107.6462\n",
      "====> Test set loss: 98.3062\n",
      "====> Epoch: 59 Average loss: 107.5621\n",
      "====> Test set loss: 98.4214\n",
      "====> Epoch: 60 Average loss: 107.4247\n",
      "====> Test set loss: 97.9914\n",
      "====> Epoch: 61 Average loss: 107.4036\n",
      "====> Test set loss: 98.1774\n",
      "====> Epoch: 62 Average loss: 107.2604\n",
      "====> Test set loss: 98.0753\n",
      "====> Epoch: 63 Average loss: 107.1274\n",
      "====> Test set loss: 97.8322\n",
      "====> Epoch: 64 Average loss: 107.0706\n",
      "====> Test set loss: 97.7345\n",
      "====> Epoch: 65 Average loss: 107.0039\n",
      "====> Test set loss: 97.6880\n",
      "====> Epoch: 66 Average loss: 106.9241\n",
      "====> Test set loss: 97.5212\n",
      "====> Epoch: 67 Average loss: 106.8548\n",
      "====> Test set loss: 97.3204\n",
      "====> Epoch: 68 Average loss: 106.7756\n",
      "====> Test set loss: 97.7307\n",
      "====> Epoch: 69 Average loss: 106.7190\n",
      "====> Test set loss: 97.3143\n",
      "====> Epoch: 70 Average loss: 106.6495\n",
      "====> Test set loss: 97.2149\n",
      "====> Epoch: 71 Average loss: 106.5326\n",
      "====> Test set loss: 97.2299\n",
      "====> Epoch: 72 Average loss: 106.5026\n",
      "====> Test set loss: 97.3809\n",
      "====> Epoch: 73 Average loss: 106.4477\n",
      "====> Test set loss: 97.1878\n",
      "====> Epoch: 74 Average loss: 106.3508\n",
      "====> Test set loss: 97.1533\n",
      "====> Epoch: 75 Average loss: 106.2826\n",
      "====> Test set loss: 97.0623\n",
      "====> Epoch: 76 Average loss: 106.1974\n",
      "====> Test set loss: 97.0417\n",
      "====> Epoch: 77 Average loss: 106.1935\n",
      "====> Test set loss: 97.0707\n",
      "====> Epoch: 78 Average loss: 106.0921\n",
      "====> Test set loss: 97.1390\n",
      "====> Epoch: 79 Average loss: 106.0033\n",
      "====> Test set loss: 97.0175\n",
      "====> Epoch: 80 Average loss: 106.0138\n",
      "====> Test set loss: 96.9264\n",
      "====> Epoch: 81 Average loss: 105.9186\n",
      "====> Test set loss: 96.6076\n",
      "====> Epoch: 82 Average loss: 105.8685\n",
      "====> Test set loss: 96.5892\n",
      "====> Epoch: 83 Average loss: 105.7872\n",
      "====> Test set loss: 96.7936\n",
      "====> Epoch: 84 Average loss: 105.7797\n",
      "====> Test set loss: 96.4805\n",
      "====> Epoch: 85 Average loss: 105.7500\n",
      "====> Test set loss: 96.6444\n",
      "====> Epoch: 86 Average loss: 105.6454\n",
      "====> Test set loss: 96.4183\n",
      "====> Epoch: 87 Average loss: 105.6541\n",
      "====> Test set loss: 96.7097\n",
      "====> Epoch: 88 Average loss: 105.5970\n",
      "====> Test set loss: 96.5112\n",
      "====> Epoch: 89 Average loss: 105.5148\n",
      "====> Test set loss: 96.4644\n",
      "====> Epoch: 90 Average loss: 105.4958\n",
      "====> Test set loss: 96.4173\n",
      "====> Epoch: 91 Average loss: 105.4461\n",
      "====> Test set loss: 96.5453\n",
      "====> Epoch: 92 Average loss: 105.3948\n",
      "====> Test set loss: 96.4992\n",
      "====> Epoch: 93 Average loss: 105.3540\n",
      "====> Test set loss: 96.2660\n",
      "====> Epoch: 94 Average loss: 105.2817\n",
      "====> Test set loss: 96.2594\n",
      "====> Epoch: 95 Average loss: 105.2176\n",
      "====> Test set loss: 96.3095\n",
      "====> Epoch: 96 Average loss: 105.1951\n",
      "====> Test set loss: 96.2622\n",
      "====> Epoch: 97 Average loss: 105.1912\n",
      "====> Test set loss: 96.1838\n",
      "====> Epoch: 98 Average loss: 105.1582\n",
      "====> Test set loss: 96.0829\n",
      "====> Epoch: 99 Average loss: 105.0434\n",
      "====> Test set loss: 96.0258\n",
      "====> Epoch: 100 Average loss: 105.0222\n",
      "====> Test set loss: 95.9073\n",
      "====> Epoch: 101 Average loss: 105.0157\n",
      "====> Test set loss: 95.7412\n",
      "====> Epoch: 102 Average loss: 104.9897\n",
      "====> Test set loss: 96.0404\n",
      "====> Epoch: 103 Average loss: 104.9463\n",
      "====> Test set loss: 95.8559\n",
      "====> Epoch: 104 Average loss: 104.9074\n",
      "====> Test set loss: 96.1272\n",
      "====> Epoch: 105 Average loss: 104.8588\n",
      "====> Test set loss: 95.9667\n",
      "====> Epoch: 106 Average loss: 104.8630\n",
      "====> Test set loss: 95.7037\n",
      "====> Epoch: 107 Average loss: 104.7820\n",
      "====> Test set loss: 95.6333\n",
      "====> Epoch: 108 Average loss: 104.8059\n",
      "====> Test set loss: 95.9317\n",
      "====> Epoch: 109 Average loss: 104.7487\n",
      "====> Test set loss: 95.5253\n",
      "====> Epoch: 110 Average loss: 104.7063\n",
      "====> Test set loss: 95.8500\n",
      "====> Epoch: 111 Average loss: 104.6636\n",
      "====> Test set loss: 95.4822\n",
      "====> Epoch: 112 Average loss: 104.6657\n",
      "====> Test set loss: 95.7243\n",
      "====> Epoch: 113 Average loss: 104.6246\n",
      "====> Test set loss: 95.5883\n",
      "====> Epoch: 114 Average loss: 104.5847\n",
      "====> Test set loss: 95.6599\n",
      "====> Epoch: 115 Average loss: 104.5276\n",
      "====> Test set loss: 95.5786\n",
      "====> Epoch: 116 Average loss: 104.5006\n",
      "====> Test set loss: 95.5240\n",
      "====> Epoch: 117 Average loss: 104.4659\n",
      "====> Test set loss: 95.5099\n",
      "====> Epoch: 118 Average loss: 104.3986\n",
      "====> Test set loss: 95.5235\n",
      "====> Epoch: 119 Average loss: 104.4307\n",
      "====> Test set loss: 95.4885\n",
      "====> Epoch: 120 Average loss: 104.3941\n",
      "====> Test set loss: 95.5618\n",
      "====> Epoch: 121 Average loss: 104.3428\n",
      "====> Test set loss: 95.4880\n",
      "====> Epoch: 122 Average loss: 104.3275\n",
      "====> Test set loss: 95.5396\n",
      "====> Epoch: 123 Average loss: 104.3088\n",
      "====> Test set loss: 95.6365\n",
      "====> Epoch: 124 Average loss: 104.2923\n",
      "====> Test set loss: 95.3792\n",
      "====> Epoch: 125 Average loss: 104.2342\n",
      "====> Test set loss: 95.4198\n",
      "====> Epoch: 126 Average loss: 104.2473\n",
      "====> Test set loss: 95.5040\n",
      "====> Epoch: 127 Average loss: 104.1678\n",
      "====> Test set loss: 95.2703\n",
      "====> Epoch: 128 Average loss: 104.1646\n",
      "====> Test set loss: 95.1187\n",
      "====> Epoch: 129 Average loss: 104.1079\n",
      "====> Test set loss: 95.2216\n",
      "====> Epoch: 130 Average loss: 104.1286\n",
      "====> Test set loss: 95.2306\n",
      "====> Epoch: 131 Average loss: 104.0780\n",
      "====> Test set loss: 95.2060\n",
      "====> Epoch: 132 Average loss: 104.0682\n",
      "====> Test set loss: 95.2037\n",
      "====> Epoch: 133 Average loss: 104.0473\n",
      "====> Test set loss: 95.3391\n",
      "====> Epoch: 134 Average loss: 104.0119\n",
      "====> Test set loss: 95.4025\n",
      "====> Epoch: 135 Average loss: 103.9512\n",
      "====> Test set loss: 95.0922\n",
      "====> Epoch: 136 Average loss: 103.9747\n",
      "====> Test set loss: 95.2373\n",
      "====> Epoch: 137 Average loss: 103.9526\n",
      "====> Test set loss: 95.2604\n",
      "====> Epoch: 138 Average loss: 103.9492\n",
      "====> Test set loss: 95.2009\n",
      "====> Epoch: 139 Average loss: 103.9186\n",
      "====> Test set loss: 94.9761\n",
      "====> Epoch: 140 Average loss: 103.8997\n",
      "====> Test set loss: 95.1620\n",
      "====> Epoch: 141 Average loss: 103.8715\n",
      "====> Test set loss: 95.1315\n",
      "====> Epoch: 142 Average loss: 103.8578\n",
      "====> Test set loss: 95.1006\n",
      "====> Epoch: 143 Average loss: 103.7966\n",
      "====> Test set loss: 95.1191\n",
      "====> Epoch: 144 Average loss: 103.8074\n",
      "====> Test set loss: 94.8411\n",
      "====> Epoch: 145 Average loss: 103.7751\n",
      "====> Test set loss: 95.0672\n",
      "====> Epoch: 146 Average loss: 103.7213\n",
      "====> Test set loss: 94.9715\n",
      "====> Epoch: 147 Average loss: 103.7091\n",
      "====> Test set loss: 94.8520\n",
      "====> Epoch: 148 Average loss: 103.6881\n",
      "====> Test set loss: 94.7323\n",
      "====> Epoch: 149 Average loss: 103.6815\n",
      "====> Test set loss: 95.0767\n",
      "====> Epoch: 150 Average loss: 103.6685\n",
      "====> Test set loss: 94.9422\n",
      "====> Epoch: 151 Average loss: 103.6623\n",
      "====> Test set loss: 94.9348\n",
      "====> Epoch: 152 Average loss: 103.6034\n",
      "====> Test set loss: 95.0459\n",
      "====> Epoch: 153 Average loss: 103.5800\n",
      "====> Test set loss: 94.6825\n",
      "====> Epoch: 154 Average loss: 103.5826\n",
      "====> Test set loss: 94.8243\n",
      "====> Epoch: 155 Average loss: 103.5656\n",
      "====> Test set loss: 94.5575\n",
      "====> Epoch: 156 Average loss: 103.5469\n",
      "====> Test set loss: 94.8687\n",
      "====> Epoch: 157 Average loss: 103.5112\n",
      "====> Test set loss: 94.8557\n",
      "====> Epoch: 158 Average loss: 103.5265\n",
      "====> Test set loss: 94.8008\n",
      "====> Epoch: 159 Average loss: 103.4950\n",
      "====> Test set loss: 94.7621\n",
      "====> Epoch: 160 Average loss: 103.4537\n",
      "====> Test set loss: 94.7001\n",
      "====> Epoch: 161 Average loss: 103.4551\n",
      "====> Test set loss: 94.8349\n",
      "====> Epoch: 162 Average loss: 103.4307\n",
      "====> Test set loss: 94.8879\n",
      "====> Epoch: 163 Average loss: 103.4211\n",
      "====> Test set loss: 94.5891\n",
      "====> Epoch: 164 Average loss: 103.4121\n",
      "====> Test set loss: 94.8179\n",
      "====> Epoch: 165 Average loss: 103.3536\n",
      "====> Test set loss: 94.4535\n",
      "====> Epoch: 166 Average loss: 103.3255\n",
      "====> Test set loss: 94.9010\n",
      "====> Epoch: 167 Average loss: 103.3005\n",
      "====> Test set loss: 94.4667\n",
      "====> Epoch: 168 Average loss: 103.3266\n",
      "====> Test set loss: 94.6932\n",
      "====> Epoch: 169 Average loss: 103.2998\n",
      "====> Test set loss: 94.9057\n",
      "====> Epoch: 170 Average loss: 103.2615\n",
      "====> Test set loss: 94.7538\n",
      "====> Epoch: 171 Average loss: 103.2462\n",
      "====> Test set loss: 94.7281\n",
      "====> Epoch: 172 Average loss: 103.2693\n",
      "====> Test set loss: 94.5666\n",
      "====> Epoch: 173 Average loss: 103.2319\n",
      "====> Test set loss: 94.6104\n",
      "====> Epoch: 174 Average loss: 103.2306\n",
      "====> Test set loss: 94.5046\n",
      "====> Epoch: 175 Average loss: 103.1738\n",
      "====> Test set loss: 94.4543\n",
      "====> Epoch: 176 Average loss: 103.2170\n",
      "====> Test set loss: 94.6262\n",
      "====> Epoch: 177 Average loss: 103.1873\n",
      "====> Test set loss: 94.6107\n",
      "====> Epoch: 178 Average loss: 103.1516\n",
      "====> Test set loss: 94.3710\n",
      "====> Epoch: 179 Average loss: 103.1306\n",
      "====> Test set loss: 94.4801\n",
      "====> Epoch: 180 Average loss: 103.1193\n",
      "====> Test set loss: 94.5153\n",
      "====> Epoch: 181 Average loss: 103.1314\n",
      "====> Test set loss: 94.3914\n",
      "====> Epoch: 182 Average loss: 103.0980\n",
      "====> Test set loss: 94.5419\n",
      "====> Epoch: 183 Average loss: 103.0837\n",
      "====> Test set loss: 94.4824\n",
      "====> Epoch: 184 Average loss: 103.0604\n",
      "====> Test set loss: 94.3782\n",
      "====> Epoch: 185 Average loss: 103.0503\n",
      "====> Test set loss: 94.5304\n",
      "====> Epoch: 186 Average loss: 103.0338\n",
      "====> Test set loss: 94.5561\n",
      "====> Epoch: 187 Average loss: 102.9948\n",
      "====> Test set loss: 94.5413\n",
      "====> Epoch: 188 Average loss: 102.9754\n",
      "====> Test set loss: 94.4869\n",
      "====> Epoch: 189 Average loss: 102.9875\n",
      "====> Test set loss: 94.5389\n",
      "====> Epoch: 190 Average loss: 102.9934\n",
      "====> Test set loss: 94.4003\n",
      "====> Epoch: 191 Average loss: 102.9616\n",
      "====> Test set loss: 94.5542\n",
      "====> Epoch: 192 Average loss: 102.9393\n",
      "====> Test set loss: 94.3020\n",
      "====> Epoch: 193 Average loss: 102.9165\n",
      "====> Test set loss: 94.3846\n",
      "====> Epoch: 194 Average loss: 102.8885\n",
      "====> Test set loss: 94.3505\n",
      "====> Epoch: 195 Average loss: 102.8959\n",
      "====> Test set loss: 94.2095\n",
      "====> Epoch: 196 Average loss: 102.8716\n",
      "====> Test set loss: 94.1646\n",
      "====> Epoch: 197 Average loss: 102.8660\n",
      "====> Test set loss: 94.4662\n",
      "====> Epoch: 198 Average loss: 102.8322\n",
      "====> Test set loss: 94.4248\n",
      "====> Epoch: 199 Average loss: 102.8510\n",
      "====> Test set loss: 94.2748\n",
      "====> Epoch: 200 Average loss: 102.8404\n",
      "====> Test set loss: 94.4842\n",
      "====> Epoch: 201 Average loss: 102.7980\n",
      "====> Test set loss: 94.4744\n",
      "====> Epoch: 202 Average loss: 102.8047\n",
      "====> Test set loss: 94.1638\n",
      "====> Epoch: 203 Average loss: 102.7634\n",
      "====> Test set loss: 94.2440\n",
      "====> Epoch: 204 Average loss: 102.7758\n",
      "====> Test set loss: 94.3202\n",
      "====> Epoch: 205 Average loss: 102.7478\n",
      "====> Test set loss: 94.3219\n",
      "====> Epoch: 206 Average loss: 102.7543\n",
      "====> Test set loss: 94.2772\n",
      "====> Epoch: 207 Average loss: 102.7129\n",
      "====> Test set loss: 94.2653\n",
      "====> Epoch: 208 Average loss: 102.7441\n",
      "====> Test set loss: 94.0766\n",
      "====> Epoch: 209 Average loss: 102.7078\n",
      "====> Test set loss: 94.0487\n",
      "====> Epoch: 210 Average loss: 102.7120\n",
      "====> Test set loss: 94.2070\n",
      "====> Epoch: 211 Average loss: 102.6575\n",
      "====> Test set loss: 94.1295\n",
      "====> Epoch: 212 Average loss: 102.6692\n",
      "====> Test set loss: 94.1007\n",
      "====> Epoch: 213 Average loss: 102.6659\n",
      "====> Test set loss: 94.0710\n",
      "====> Epoch: 214 Average loss: 102.6348\n",
      "====> Test set loss: 94.1524\n",
      "====> Epoch: 215 Average loss: 102.6317\n",
      "====> Test set loss: 93.8962\n",
      "====> Epoch: 216 Average loss: 102.5794\n",
      "====> Test set loss: 94.1844\n",
      "====> Epoch: 217 Average loss: 102.6205\n",
      "====> Test set loss: 94.3922\n",
      "====> Epoch: 218 Average loss: 102.5880\n",
      "====> Test set loss: 94.0479\n",
      "====> Epoch: 219 Average loss: 102.5537\n",
      "====> Test set loss: 93.9334\n",
      "====> Epoch: 220 Average loss: 102.5424\n",
      "====> Test set loss: 93.9283\n",
      "====> Epoch: 221 Average loss: 102.5295\n",
      "====> Test set loss: 94.0985\n",
      "====> Epoch: 222 Average loss: 102.5471\n",
      "====> Test set loss: 94.1617\n",
      "====> Epoch: 223 Average loss: 102.5261\n",
      "====> Test set loss: 94.1677\n",
      "====> Epoch: 224 Average loss: 102.4996\n",
      "====> Test set loss: 94.0260\n",
      "====> Epoch: 225 Average loss: 102.5072\n",
      "====> Test set loss: 93.9571\n",
      "====> Epoch: 226 Average loss: 102.4814\n",
      "====> Test set loss: 93.9069\n",
      "====> Epoch: 227 Average loss: 102.5077\n",
      "====> Test set loss: 93.9430\n",
      "====> Epoch: 228 Average loss: 102.4622\n",
      "====> Test set loss: 93.9066\n",
      "====> Epoch: 229 Average loss: 102.4805\n",
      "====> Test set loss: 94.1042\n",
      "====> Epoch: 230 Average loss: 102.4445\n",
      "====> Test set loss: 94.0440\n",
      "====> Epoch: 231 Average loss: 102.4666\n",
      "====> Test set loss: 94.0368\n",
      "====> Epoch: 232 Average loss: 102.4042\n",
      "====> Test set loss: 93.9433\n",
      "====> Epoch: 233 Average loss: 102.4158\n",
      "====> Test set loss: 93.6976\n",
      "====> Epoch: 234 Average loss: 102.3880\n",
      "====> Test set loss: 93.9172\n",
      "====> Epoch: 235 Average loss: 102.3780\n",
      "====> Test set loss: 93.9566\n",
      "====> Epoch: 236 Average loss: 102.3556\n",
      "====> Test set loss: 93.7747\n",
      "====> Epoch: 237 Average loss: 102.3587\n",
      "====> Test set loss: 93.9683\n",
      "====> Epoch: 238 Average loss: 102.3503\n",
      "====> Test set loss: 94.0375\n",
      "====> Epoch: 239 Average loss: 102.3752\n",
      "====> Test set loss: 94.0294\n",
      "====> Epoch: 240 Average loss: 102.3342\n",
      "====> Test set loss: 93.6801\n",
      "====> Epoch: 241 Average loss: 102.3423\n",
      "====> Test set loss: 94.0686\n",
      "====> Epoch: 242 Average loss: 102.3208\n",
      "====> Test set loss: 94.0599\n",
      "====> Epoch: 243 Average loss: 102.2836\n",
      "====> Test set loss: 93.8884\n",
      "====> Epoch: 244 Average loss: 102.2784\n",
      "====> Test set loss: 93.9892\n",
      "====> Epoch: 245 Average loss: 102.2707\n",
      "====> Test set loss: 93.7091\n",
      "====> Epoch: 246 Average loss: 102.2980\n",
      "====> Test set loss: 93.8917\n",
      "====> Epoch: 247 Average loss: 102.2652\n",
      "====> Test set loss: 93.9799\n",
      "====> Epoch: 248 Average loss: 102.2400\n",
      "====> Test set loss: 93.7112\n",
      "====> Epoch: 249 Average loss: 102.2098\n",
      "====> Test set loss: 93.8513\n",
      "====> Epoch: 250 Average loss: 102.2339\n",
      "====> Test set loss: 94.0114\n",
      "====> Epoch: 251 Average loss: 102.2292\n",
      "====> Test set loss: 93.7355\n",
      "====> Epoch: 252 Average loss: 102.1758\n",
      "====> Test set loss: 94.0328\n",
      "====> Epoch: 253 Average loss: 102.1956\n",
      "====> Test set loss: 93.7689\n",
      "====> Epoch: 254 Average loss: 102.1732\n",
      "====> Test set loss: 93.8503\n",
      "====> Epoch: 255 Average loss: 102.1759\n",
      "====> Test set loss: 93.8498\n",
      "====> Epoch: 256 Average loss: 102.1822\n",
      "====> Test set loss: 93.9054\n",
      "====> Epoch: 257 Average loss: 102.1830\n",
      "====> Test set loss: 93.6730\n",
      "====> Epoch: 258 Average loss: 102.1528\n",
      "====> Test set loss: 93.8932\n",
      "====> Epoch: 259 Average loss: 102.1511\n",
      "====> Test set loss: 93.6965\n",
      "====> Epoch: 260 Average loss: 102.1270\n",
      "====> Test set loss: 93.4863\n",
      "====> Epoch: 261 Average loss: 102.1320\n",
      "====> Test set loss: 93.7279\n",
      "====> Epoch: 262 Average loss: 102.0957\n",
      "====> Test set loss: 93.6025\n",
      "====> Epoch: 263 Average loss: 102.0686\n",
      "====> Test set loss: 93.7267\n",
      "====> Epoch: 264 Average loss: 102.0839\n",
      "====> Test set loss: 93.8368\n",
      "====> Epoch: 265 Average loss: 102.0347\n",
      "====> Test set loss: 93.8594\n",
      "====> Epoch: 266 Average loss: 102.0705\n",
      "====> Test set loss: 93.7177\n",
      "====> Epoch: 267 Average loss: 102.0648\n",
      "====> Test set loss: 93.6089\n",
      "====> Epoch: 268 Average loss: 102.0112\n",
      "====> Test set loss: 93.7523\n",
      "====> Epoch: 269 Average loss: 102.0523\n",
      "====> Test set loss: 93.7124\n",
      "====> Epoch: 270 Average loss: 102.0473\n",
      "====> Test set loss: 93.5696\n",
      "====> Epoch: 271 Average loss: 102.0155\n",
      "====> Test set loss: 93.6368\n",
      "====> Epoch: 272 Average loss: 102.0404\n",
      "====> Test set loss: 93.7971\n",
      "====> Epoch: 273 Average loss: 101.9952\n",
      "====> Test set loss: 93.6031\n",
      "====> Epoch: 274 Average loss: 102.0050\n",
      "====> Test set loss: 93.5210\n",
      "====> Epoch: 275 Average loss: 102.0005\n",
      "====> Test set loss: 93.6725\n",
      "====> Epoch: 276 Average loss: 101.9850\n",
      "====> Test set loss: 93.7433\n",
      "====> Epoch: 277 Average loss: 101.9718\n",
      "====> Test set loss: 93.5947\n",
      "====> Epoch: 278 Average loss: 101.9685\n",
      "====> Test set loss: 93.5528\n",
      "====> Epoch: 279 Average loss: 101.9536\n",
      "====> Test set loss: 93.7572\n",
      "====> Epoch: 280 Average loss: 101.9255\n",
      "====> Test set loss: 93.7351\n",
      "====> Epoch: 281 Average loss: 101.9323\n",
      "====> Test set loss: 93.7624\n",
      "====> Epoch: 282 Average loss: 101.9087\n",
      "====> Test set loss: 93.5125\n",
      "====> Epoch: 283 Average loss: 101.9132\n",
      "====> Test set loss: 93.7259\n",
      "====> Epoch: 284 Average loss: 101.9273\n",
      "====> Test set loss: 93.5258\n",
      "====> Epoch: 285 Average loss: 101.8924\n",
      "====> Test set loss: 93.5411\n",
      "====> Epoch: 286 Average loss: 101.8896\n",
      "====> Test set loss: 93.5140\n",
      "====> Epoch: 287 Average loss: 101.9052\n",
      "====> Test set loss: 93.4730\n",
      "====> Epoch: 288 Average loss: 101.8836\n",
      "====> Test set loss: 93.4323\n",
      "====> Epoch: 289 Average loss: 101.8709\n",
      "====> Test set loss: 93.6513\n",
      "====> Epoch: 290 Average loss: 101.8480\n",
      "====> Test set loss: 93.6106\n",
      "====> Epoch: 291 Average loss: 101.8476\n",
      "====> Test set loss: 93.6361\n",
      "====> Epoch: 292 Average loss: 101.8182\n",
      "====> Test set loss: 93.4027\n",
      "====> Epoch: 293 Average loss: 101.8178\n",
      "====> Test set loss: 93.5208\n",
      "====> Epoch: 294 Average loss: 101.8399\n",
      "====> Test set loss: 93.4886\n",
      "====> Epoch: 295 Average loss: 101.8008\n",
      "====> Test set loss: 93.3664\n",
      "====> Epoch: 296 Average loss: 101.7784\n",
      "====> Test set loss: 93.5101\n",
      "====> Epoch: 297 Average loss: 101.7844\n",
      "====> Test set loss: 93.5207\n",
      "====> Epoch: 298 Average loss: 101.7924\n",
      "====> Test set loss: 93.3936\n",
      "====> Epoch: 299 Average loss: 101.8058\n",
      "====> Test set loss: 93.6625\n",
      "====> Epoch: 300 Average loss: 101.7456\n",
      "====> Test set loss: 93.3804\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training and testing the VAE\n",
    "\n",
    "epochs = 300\n",
    "codes = dict(μ=list(), logσ2=list(), y=list())\n",
    "for epoch in range(0, epochs + 1):\n",
    "    # Training\n",
    "    if epoch > 0:  # test untrained net first\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            # ===================forward=====================\n",
    "            x_hat, mu, logvar = model(x)\n",
    "            loss = loss_function(x_hat, x, mu, logvar)\n",
    "            train_loss += loss.item()\n",
    "            # ===================backward====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # ===================log========================\n",
    "        print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "    \n",
    "    # Testing\n",
    "    \n",
    "    means, logvars, labels = list(), list(), list()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            # ===================forward=====================\n",
    "            x_hat, mu, logvar = model(x)\n",
    "            test_loss += loss_function(x_hat, x, mu, logvar).item()\n",
    "            # =====================log=======================\n",
    "            means.append(mu.detach())\n",
    "            logvars.append(logvar.detach())\n",
    "            labels.append(y.detach())\n",
    "    # ===================log========================\n",
    "    codes['μ'].append(torch.cat(means))\n",
    "    codes['logσ2'].append(torch.cat(logvars))\n",
    "    codes['y'].append(torch.cat(labels))\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'====> Test set loss: {test_loss:.4f}')\n",
    "\n",
    "display_images(x, x_hat, 1, f'Epoch {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a few samples\n",
    "\n",
    "N = 16\n",
    "z = torch.randn((N, d)).to(device)\n",
    "sample = model.decoder(z)\n",
    "display_images(None, sample, N // 4, count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display last test batch\n",
    "\n",
    "display_images(None, x, 4, count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose starting and ending point for the interpolation -> shows original and reconstructed\n",
    "\n",
    "A, B = 1, 14\n",
    "sample = model.decoder(torch.stack((mu[A].data, mu[B].data), 0))\n",
    "display_images(None, torch.stack(((\n",
    "    x[A].data.view(-1),\n",
    "    x[B].data.view(-1),\n",
    "    sample.data[0],\n",
    "    sample.data[1]\n",
    ")), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an interpolation between input A and B, in N steps\n",
    "\n",
    "N = 16\n",
    "code = torch.Tensor(N, 20).to(device)\n",
    "sample = torch.Tensor(N, 28, 28).to(device)\n",
    "for i in range(N):\n",
    "    code[i] = i / (N - 1) * mu[B].data + (1 - i / (N - 1) ) * mu[A].data\n",
    "    # sample[i] = i / (N - 1) * x[B].data + (1 - i / (N - 1) ) * x[A].data\n",
    "sample = model.decoder(code)\n",
    "display_images(None, sample, N // 4, count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from res.plot_lib import set_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default(figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, E = list(), list(), list()  # input, classes, embeddings\n",
    "N = 1000  # samples per epoch\n",
    "epochs = (0, 5, 10)\n",
    "for epoch in epochs:\n",
    "    X.append(codes['μ'][epoch][:N])\n",
    "    E.append(TSNE(n_components=2).fit_transform(X[-1].detach().cpu()))\n",
    "    Y.append(codes['y'][epoch][:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(ncols=3)\n",
    "for i, e in enumerate(epochs):\n",
    "    s = a[i].scatter(E[i][:,0], E[i][:,1], c=Y[i], cmap='tab10')\n",
    "    a[i].grid(False)\n",
    "    a[i].set_title(f'Epoch {e}')\n",
    "    a[i].axis('equal')\n",
    "f.colorbar(s, ax=a[:], ticks=np.arange(10), boundaries=np.arange(11) - .5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
